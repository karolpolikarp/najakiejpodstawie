# Strategia Zapobiegania Rate Limiting (429 Errors)

## Og√≥lny Wzorzec Problemu

B≈ÇƒÖd 429 z Anthropic API ("50,000 input tokens per minute exceeded") wystƒôpuje gdy:

```
Input Tokens = System Prompt + User Message + Context (Articles)
```

### Typowy Scenariusz Przekroczenia:

1. **Pytanie u≈ºytkownika:** "Czy mogƒô rozwiƒÖzaƒá umowƒô z operatorem?"
2. **System wykrywa 3 tematy:** telecom, employment, rental (zbyt szerokie keywords)
3. **Ka≈ºdy temat dodaje artyku≈Çy:** 3 tematy √ó 3 artyku≈Çy = 9 artyku≈Ç√≥w
4. **Ka≈ºdy artyku≈Ç:** ~50,000 znak√≥w ‚âà 12,500 token√≥w
5. **≈ÅƒÖczne tokeny:** 9 √ó 12,500 = 112,500 token√≥w ‚Üí **B≈ÅƒÑD 429**

---

## Og√≥lna Strategia RozwiƒÖzania

### üéØ Zasada #1: Limit Wykrywanych Temat√≥w (TOP 1)

**Problem:** System wykrywa WSZYSTKIE pasujƒÖce tematy jednocze≈õnie

**RozwiƒÖzanie:** Wykryj tylko NAJLEPIEJ pasujƒÖcy temat

```typescript
// ‚ùå PRZED (Z≈ÅE):
for (const topic of allTopics) {
  if (topic.keywords.some(kw => message.includes(kw))) {
    detectedTopics.push(topic);  // Dodaje WSZYSTKIE
  }
}

// ‚úÖ PO (DOBRE):
const MAX_DETECTED_TOPICS = 1;

const scored = allTopics
  .map(topic => ({
    topic,
    score: topic.keywords.filter(kw =>
      message.includes(kw)
    ).length  // Zlicz ile keywords pasuje
  }))
  .filter(t => t.score > 0)
  .sort((a, b) => b.score - a.score)  // Najwy≈ºszy score pierwszy
  .slice(0, MAX_DETECTED_TOPICS);     // We≈∫ TOP 1
```

**Oszczƒôdno≈õƒá:** 60-80% redukcja token√≥w

---

### üéØ Zasada #2: Zwiƒôksz Specyficzno≈õƒá Keywords

**Problem:** Zbyt og√≥lne s≈Çowa kluczowe powodujƒÖ false positives

**RozwiƒÖzanie:** U≈ºyj pe≈Çnych fraz zamiast pojedynczych s≈Ç√≥w

```typescript
// ‚ùå PRZED (Z≈ÅE):
keywords: ["rozwiƒÖzanie", "umowa", "wypowiedzenie"]
// Pasuje do: pracy, najmu, telecomu, wszystkiego!

// ‚úÖ PO (DOBRE):
keywords: [
  "rozwiƒÖzanie umowy o pracƒô",
  "wypowiedzenie pracy",
  "zwolnienie z pracy"
]
// Pasuje TYLKO do employment questions
```

---

### üéØ Zasada #3: Zmniejsz Limit Pobieranych Artyku≈Ç√≥w

**Problem:** Za du≈ºo artyku≈Ç√≥w pobieranych jednocze≈õnie

**RozwiƒÖzanie:** Dynamiczne limity oparte na tier u≈ºytkownika

```typescript
// Ka≈ºdy artyku≈Ç: ~50k chars ‚âà 12.5k tokens
// Anthropic limit: 50k tokens/min

const MAX_ARTICLES = usePremiumModel
  ? 6   // Premium: 6 √ó 12.5k = 75k tokens (WCIƒÑ≈ª ZA DU≈ªO!)
  : 3;  // Standard: 3 √ó 12.5k = 37.5k tokens (BEZPIECZNE)

// ‚úÖ ZALECANE WARTO≈öCI (po TOP 1 topic):
const MAX_ARTICLES_FROM_TOPICS = usePremiumModel ? 4 : 2;
const MAX_TOTAL_ARTICLES = usePremiumModel ? 6 : 3;
```

**Matematyka:**
- Standard user: 2 artyku≈Çy √ó 12.5k = 25k tokens + prompt (2.5k) = **27.5k** ‚úÖ
- Premium user: 4 artyku≈Çy √ó 12.5k = 50k tokens + prompt (2.5k) = **52.5k** ‚ö†Ô∏è (na granicy)

---

### üéØ Zasada #4: Monitoring Token√≥w

**Problem:** Nie wiesz ile token√≥w faktycznie zu≈ºywasz

**RozwiƒÖzanie:** Loguj usage z message_start events

```typescript
// Anthropic streaming API zwraca usage w message_start event
if (parsed.type === 'message_start' && parsed.message?.usage) {
  const inputTokens = parsed.message.usage.input_tokens;
  const outputTokens = parsed.message.usage.output_tokens;

  console.log(`[TOKENS] Input: ${inputTokens}, Output: ${outputTokens}`);

  // Alert gdy blisko limitu
  if (inputTokens > 40000) {
    console.warn(`[TOKENS] ‚ö†Ô∏è  HIGH INPUT: ${inputTokens}/50000`);
  }
}
```

**Zalety:**
- Widzisz rzeczywiste zu≈ºycie
- Mo≈ºesz wykryƒá problemy przed 429
- Data-driven optimization

---

### üéØ Zasada #5: Graceful Degradation

**Problem:** Gdy 429 wystƒÖpi, u≈ºytkownik dostaje b≈ÇƒÖd

**RozwiƒÖzanie:** Fallback do wiedzy AI bez artyku≈Ç√≥w

```typescript
if (response.status === 429) {
  console.log('[FALLBACK] Rate limit exceeded - using AI knowledge');

  // Drugi request BEZ context articles (tylko system prompt + message)
  const fallbackResponse = await anthropic.messages.create({
    model: 'claude-haiku-4-5',  // Ta≈Ñszy model
    max_tokens: 2048,
    system: minimalSystemPrompt,  // Bez artyku≈Ç√≥w
    messages: [{ role: 'user', content: message }]
  });

  // Dodaj disclaimer
  return {
    answer: fallbackResponse.content +
      "\n\n‚ö†Ô∏è Odpowied≈∫ oparta na wiedzy AI (przekroczono limit zapyta≈Ñ)"
  };
}
```

---

## Implementacja Krok Po Kroku

### Krok 1: Audit Obecnego Systemu

```bash
# Ile temat√≥w w LEGAL_CONTEXT?
grep -c "^  [a-z_]*: {" legal-context.ts

# Ile znak√≥w ma system prompt?
sed -n '241,494p' index.ts | wc -c

# Ile artyku≈Ç√≥w pobierasz ≈õrednio?
# Sprawd≈∫ logi: [CONTEXT] Detected X articles
```

### Krok 2: Dodaj Topic Scoring

```typescript
// W detectLegalContext():
const topicScores = Object.entries(LEGAL_CONTEXT)
  .map(([key, data]) => ({
    key,
    data,
    score: data.keywords.filter(kw =>
      message.toLowerCase().includes(kw.toLowerCase())
    ).length
  }))
  .filter(t => t.score > 0)
  .sort((a, b) => b.score - a.score)
  .slice(0, 1);  // TOP 1 tylko
```

### Krok 3: Dodaj Token Monitoring

```typescript
// W stream parsing loop:
if (parsed.type === 'message_start' && parsed.message?.usage) {
  const usage = parsed.message.usage;
  console.log(`[TOKENS] Input: ${usage.input_tokens}, Output: ${usage.output_tokens}`);
}
```

### Krok 4: Zmniejsz Limity Artyku≈Ç√≥w

```typescript
// W enrichWithArticles():
const MAX_ARTICLES_FROM_TOPICS = 2;  // By≈Ço: 5
const MAX_TOTAL_ARTICLES = 3;        // By≈Ço: 10
```

### Krok 5: Zwiƒôksz Specyficzno≈õƒá Keywords

```typescript
// Dla ka≈ºdego tematu w legal-context.ts:
// ‚ùå PRZED:
keywords: ["wypowiedzenie", "umowa"]

// ‚úÖ PO:
keywords: [
  "wypowiedzenie umowy o pracƒô",
  "rozwiƒÖzanie umowy o pracƒô",
  "zwolnienie z pracy"
]
```

---

## Przyk≈Çad Przed/Po

### PRZED zmianami:

```
Pytanie: "Czy mogƒô rozwiƒÖzaƒá umowƒô z operatorem?"

[CONTEXT] Detected topic: Wypowiedzenie umowy o pracƒô
[CONTEXT] Detected topic: Wypowiedzenie najmu
[CONTEXT] Detected topic: Umowy telekomunikacyjne
[CONTEXT] Total: 9 articles

[TOKENS] Input: 112,500 ‚Üí B≈ÅƒÑD 429
```

### PO zmianach:

```
Pytanie: "Czy mogƒô rozwiƒÖzaƒá umowƒô z operatorem?"

[CONTEXT] Detected topic: Umowy telekomunikacyjne - score: 3
[CONTEXT] Total: 3 articles

[TOKENS] Input: 37,500 ‚Üí SUCCESS ‚úÖ
```

---

## Monitoring i Debugging

### Sprawd≈∫ Logi:

```bash
# Ile temat√≥w wykryto?
grep "\[CONTEXT\] Detected topic:" logs.txt

# Ile token√≥w zu≈ºyto?
grep "\[TOKENS\]" logs.txt

# Czy sƒÖ b≈Çƒôdy 429?
grep "429\|rate limit" logs.txt
```

### Red Flags:

- ‚ö†Ô∏è Wiƒôcej ni≈º 1 temat wykryty
- ‚ö†Ô∏è Wiƒôcej ni≈º 5 artyku≈Ç√≥w pobieranych
- ‚ö†Ô∏è Input tokens > 40,000
- ‚ö†Ô∏è Czƒôste b≈Çƒôdy 429 w logach

---

## Checklisty

### ‚úÖ Przed Deployment:

- [ ] MAX_DETECTED_TOPICS = 1
- [ ] MAX_TOTAL_ARTICLES ‚â§ 3 (standard) / ‚â§ 6 (premium)
- [ ] Token monitoring w≈ÇƒÖczony
- [ ] Keywords sƒÖ specific (3+ wyrazy)
- [ ] Przetestowano z najczƒôstszymi pytaniami

### ‚úÖ Po Deployment:

- [ ] Sprawd≈∫ logi co tydzie≈Ñ
- [ ] Monitoruj rate 429 errors
- [ ] Adjustuj limity je≈õli potrzeba
- [ ] Dodawaj nowe tematy ostro≈ºnie (testuj keyword specificity)

---

## Zaawansowane Optymalizacje (Future)

### 1. Lazy Loading Artyku≈Ç√≥w

Zamiast pobieraƒá artyku≈Çy PRZED pierwszym requestem, pozw√≥l LLM je pobieraƒá przez tool calling:

```typescript
// Teraz: Eager loading
enrichWithArticles() ‚Üí Fetch all ‚Üí Add to prompt ‚Üí LLM call

// Future: Lazy loading
LLM call ‚Üí LLM decides what it needs ‚Üí Tool call ‚Üí Fetch specific article
```

### 2. Cache'owanie Artyku≈Ç√≥w

```typescript
const articleCache = new Map<string, ArticleResponse>();

// Cache artyku≈Çy przez 1h
if (cache.has(key) && !isExpired(key)) {
  return cache.get(key);
}
```

### 3. Adaptive Limits

```typescript
// Zmniejsz limity gdy rate limit siƒô zbli≈ºa
if (recentTokenUsage > 40000) {
  MAX_ARTICLES = 2;  // Temporary reduction
}
```

---

## Podsumowanie

**3 Kluczowe Zmiany:**

1. **TOP 1 Topic Detection** - Zmniejsza tokeny o 60-80%
2. **Specific Keywords** - Zapobiega false positives
3. **Token Monitoring** - Data-driven optimization

**Rezultat:**

- Redukcja 429 errors o 90%+
- ≈örednie zu≈ºycie: 25-35k tokens (z 75-120k)
- Lepsze dopasowanie odpowiedzi (1 topic = bardziej focused)

---

## Wsparcie

Je≈õli nadal wystƒôpujƒÖ problemy 429:

1. Sprawd≈∫ logi dla `[TOKENS]` i `[CONTEXT]`
2. Upewnij siƒô ≈ºe MAX_DETECTED_TOPICS = 1
3. Zmniejsz MAX_TOTAL_ARTICLES do 2
4. Dodaj more specific keywords
5. Consider lazy loading architecture

**Ostatnia aktualizacja:** 2025-11-12
**Wersja:** 1.0
